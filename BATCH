#!/bin/bash

# Copy/paste this job script into a text file and submit with the command:
#    sbatch thefilename
# job standard output will go to the file slurm-%j.out (where %j is the job ID)

#SBATCH --time=00:15:00   # walltime limit (HH:MM:SS)
#SBATCH --nodes=2   # number of nodes
#SBATCH --ntasks-per-node=4   # We use 2 tasks per node for this workload
#SBATCH --job-name="cuda_gemm"
#SBATCH --mem=16G   # maximum memory per node
#SBATCH --partition=instruction
#SBATCH --gres=gpu:a100:1

# LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE

module load cuda openmpi

export CUDA_VISIBLE_DEVICES=0
# run 2 threads per node
mpirun -np 4 ./mmult
echo $(hostname)

